We explore generating new paper titles given past titles on arXiv.
We first explore generating titles conditioned on a specific author (using GPT-3 without finetuning).
We then generate titles conditioned only on their publication year (using GPT-Neo with finetuning)

## Author-specific paper titles (prompting gpt3 text-davinci-002)
To generate author-specific titles, we take the five most recent titles from each author with atleast 3 arXiv AI papers (cs.ML, cs.LG, stat.ML).
We then feed then format the papers using the following template and query for the next title using GPT-3 with the OpenAI API:

```
Here is a list of related machine-learning papers:

> [title 1]
> [title 2]
...
> [title 5]
> ____
```

See the results in [the demo](http://localhost:4000/docs/#demo:~:text=Type%20in%20the%20name%20of%20an%20author%20to%20see%20the%20predicted%20titles%20of%20their%20future%20papers) above or the full results in this [json file](https://github.com/csinva/gpt-paper-title-generator/blob/master/samples/gpt3/authors_save_full.json).
Note: paper titles with irregular formatting were likely removed and different authors with exactly the same were merged 🙁

Here's a concrete example -- when prompting with these 5 recent titles:

```
> Hierarchical Shrinkage: improving the accuracy and interpretability of tree-based methods
> Fast Interpretable Greedy-Tree Sums (FIGS)
> Adaptive wavelet distillation from neural networks through interpretations
> Emb-GAM: an Interpretable and Efficient Predictor using Pre-trained Language Models
> Explaining Patterns in Data with Language Models via Interpretable Autoprompting
> ____
```

We get these 5 (independent) random generations for the blank:

```
1. Towards Interpretable Natural Language Processing: A Survey
2. A Unified Framework for Interpretable Machine Learning
3. Compositional Attention Networks for Machine Reasoning
4. Achieving Open Vocabulary Neural Machine Translation
5. A Deep Understanding of Neural Networks through Deep Visualization
```

The results are often interesting but fall into failure modes where they generate irrelevant titles for an author, often leaning towards popular topics such as deep learning, multi-task learning, and reinforcement learning.
Note: the model used was GPT `text-davinci-002` on Oct 14 2022. It likely was not up to date with the most current advances and could be improved with finetuning on more recent titles. We explore that below with a smaller model:

## Finetuned paper title generation (gptneo 2.7B)

To improve the model, we now turn to finetuning a model specifically for paper-title generation. We start from the [gpt-neo-2.7B checkpoint](https://huggingface.co/EleutherAI/gpt-neo-2.7B) (see the [the training script](https://github.com/csinva/gpt-paper-title-generator/blob/91d8aa78d83f16778a120ec4a3dc41be28f5e8f2/gptneo/02_finetune_hf.py) for hyperparameters). We finetune on all [paper titles on arXiv](https://www.kaggle.com/datasets/Cornell-University/arxiv) in the categories cs.AI, cs.LG, stat.ML. However, we exclude some titles for testing: we exclude all papers after Apr 1, 2022 and an additional random 5\% of titles. Note that papers are very skewed towards recent years:

![](https://csinva.io/gpt-paper-title-generator/figs/paper_metadata.svg)

We also exclude titles with a length of less than 6 words or greater than 20 words. This results in 98,388 papers for finetuning.

**Samples**
Here are some examples of titles generated by the model (see a large dump in [this folder](https://github.com/csinva/gpt-paper-title-generator/tree/master/samples/gptneo)):

**2021**
```
'Learning to See: Disentangled Vision and Image Segmentation with Domain-Guided Domain Attribute Learning'
"A New Benchmark for Evaluating Neural Networks' Fairness, Precision, and Explainability"
'Improving Model Robustness through Deep Learning for Online Vehicle Maintenance'
'Multi-Task Learning via Sparse Sampling: Applying Differentiable Multi-Agent Reinforcement Learning'
'Modeling the Impact of Context-Aware Training on Spatiotemporal GAN'
'Self-Attention for Multi-Task Learning of Graph Embeddings for Structural Prediction'
'A machine, a method and a message all can be equally effective? Testing the message-passing algorithm with binary classification'
'Adversarial Reasoning: When Does Knowledge Transfer Really Transfer?'
'Robustness against Random Attacks against High-speed Single-link Channel Spacing Modulation (HS-SCMM)'
'Robustness analysis in real-world scenarios with a multi-agent framework'
'Dynamic Temporal-Difference Recurrent Neural Networks for Motion-Adaptive Driving: A Benchmark'
'Inferring Multiple Event Types From Fewer Sources: Causal Inference with a Fewer Observations'
```

**2022**

**2023**


**Inference example**
We've released our finetuned model if you want to play with it:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
model = AutoModelForCausalLM.from_pretrained("csinva/gpt-neo-2.7B-titles")
tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neo-2.7B")
pipe = pipeline('text-generation', model=model, tokenizer=tokenizer)
pipe('2022\n\n')
------------------
> [{'generated_text': '2022\n\n Automating the Visualization of Convolutional Neural Networks\n'}]
```

During finetuning each paper title was given in the format `<year>\n\n <title>\n` (e.g. `2020\n\n Interpretations are useful: penalizing explanations to align neural networks with prior knowledge\n`). The same format should be used for inference. These samples are considerably improved over the samples we made with GPT2 [back in 2019](https://csinva.io/gpt-paper-title-generator/web/gpt2) (the good old days).

## Some possible followups

- Use information about abstracts instead of just titles
- Improve author-specific title generation with finetuning (some authors have *a lot* of papers)

![](https://csinva.io/gpt-paper-title-generator/figs/author_counts.svg)

- Build classifier of paper year and then use [iPrompt](https://arxiv.org/abs/2210.01848) to describe the year-to-year differences

## Reference

- Code
    - to rerun, first run the code in the `scrape` folder
    - then pick one of the model folders (e.g. `gptneo`) and run the notebooks in that folder
      - this will add results into the `samples` folder
- arXiv dataset from [here](https://www.kaggle.com/datasets/Cornell-University/arxiv)
- adorable robot from [here](http://pngimg.com/uploads/robot/robot_PNG94.png)