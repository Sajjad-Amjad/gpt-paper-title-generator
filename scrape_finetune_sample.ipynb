{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape data\n",
    "in order to work, we require that the data is saved to a csv file with one piece of text (e.g. one paper title) on each line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**scrape arxiv for titles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxivscraper as ax\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "# scraper for arxiv stat.ml\n",
    "scraper = ax.Scraper(category='stat', date_from='2017-08-01',\n",
    "                     date_until='2019-07-01', t=10, \n",
    "                     filters={'categories':['stat.ml'],'abstract':['learning']})\n",
    "\n",
    "# scraper for arxiv q-bio\n",
    "scraper = ax.Scraper(category='q-bio', date_from='2016-08-01',\n",
    "                     date_until='2019-07-01', t=10, \n",
    "                     filters={'categories':['q-bio.GN', 'q-bio.NC']})\n",
    "'''\n",
    "# scraper for arxiv physics\n",
    "scraper = ax.Scraper(category='physics', date_from='2019-05-01',\n",
    "                     date_until='2019-07-03', t=10,\n",
    "                     filters={'categories':['quant-ph']})\n",
    "\n",
    "output = scraper.scrape()\n",
    "\n",
    "\n",
    "# cols = ('id', 'title', 'categories', 'abstract', 'doi', 'created', 'updated', 'authors')\n",
    "titles = [' '.join(o['title'].split()) for o in output]\n",
    "np.savetxt('titles.csv', np.array(titles), fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alternatively, scrape something else**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "\n",
    "# scrape some interesting quotes\n",
    "url = 'https://raw.githubusercontent.com/akhiltak/inspirational-quotes/master/Quotes.csv'\n",
    "response = urllib.request.urlopen(url).read().decode()\n",
    "quotes = []\n",
    "lines = response.split('\\n')\n",
    "for line in lines[:-1]:\n",
    "    quotes.append(line.split(';')[0].replace(\"\\'\", '').replace('*', '').replace('#', '').replace('%', '').replace('&', ''))\n",
    "    \n",
    "np.savetxt('titles.csv', np.array(quotes[1:]), fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g. could scrape tweets (requires having twitter api credentials)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#Twitter API credentials\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_key = \"\"\n",
    "access_secret = \"\"\n",
    "\n",
    "\n",
    "def get_all_tweets(screen_name):\n",
    "    #Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "    \n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    \n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []  \n",
    "    \n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
    "    \n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "    \n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "    \n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        print(f\"getting tweets before {oldest}\")\n",
    "        \n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
    "        \n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "        \n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "        \n",
    "        print(f\"...{len(alltweets)} tweets downloaded so far\")\n",
    "    \n",
    "    #transform the tweepy tweets into a 2D array that will populate the csv \n",
    "    outtweets = [[tweet.id_str, tweet.created_at, tweet.text] for tweet in alltweets]\n",
    "    \n",
    "    #write the csv  \n",
    "    with open(f'new_{screen_name}_tweets.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\",\"created_at\",\"text\"])\n",
    "        writer.writerows(outtweets)\n",
    "    \n",
    "    pass\n",
    "\n",
    "def clean_csv(fname='new_SICKOFWOLVES_tweets.csv'):\n",
    "    df = pd.read_csv(fname)\n",
    "    df = df[~df['text'].str.contains('STORE')]\n",
    "    df = df[~df['text'].str.startswith('@')]\n",
    "    df['text'] = df['text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0]) # remove urls\n",
    "    df['text'].to_csv('data/wolves_tweets.csv', index=False)\n",
    "# get_all_tweets(\"SICKOFWOLVES\")\n",
    "clean_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finetune gpt2\n",
    "this code will download gpt2 and finetune it on the file title.csv, generating samples at intermediate steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "\n",
    "model_name = \"117M\"\n",
    "gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/117M/\n",
    "\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.finetune(sess,\n",
    "              'titles.csv',\n",
    "              model_name=model_name,\n",
    "              steps=1000,\n",
    "              save_every=200,\n",
    "              sample_every=25)   # steps is max number of training steps\n",
    "\n",
    "gpt2.generate(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# look at some samples\n",
    "the samples are saved to the 'samples' folder by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = 'samples/samples-901'\n",
    "t = open(sample_file, 'r').read()\n",
    "\n",
    "for s in ['endoftext', 'startoftext', '<|', '|>']:\n",
    "    t = t.replace(s, '')\n",
    "for title in t.title().split('\\n')[1:]:\n",
    "    if not title == '':\n",
    "        print('- ' + title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating new samples from the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generate one sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Source Separation Via Non-Negative Eigenvector Field Variate Operator\n"
     ]
    }
   ],
   "source": [
    "prefix = 'neural' # None is default\n",
    "text = gpt2.generate(sess,\n",
    "              length=40,\n",
    "              temperature=0.7,\n",
    "              prefix=neural,\n",
    "              nsamples=1,\n",
    "              batch_size=1,\n",
    "              return_as_list=True\n",
    "             )\n",
    "\n",
    "\n",
    "t = text[0].title()\n",
    "t = t.replace('<|Startoftext|>', '').replace('\\n', '') # remove extraneous stuff\n",
    "t = t[:t.index('<|Endoftext|>')] # only get one title\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generate a bunch of samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = gpt2.generate(sess,\n",
    "#               length=40,\n",
    "              temperature=0.7,\n",
    "              prefix=None,\n",
    "              nsamples=100,\n",
    "              batch_size=1,\n",
    "              return_as_list=True\n",
    "             )\n",
    "\n",
    "\n",
    "t = text[0].title()\n",
    "t = t.replace('<|Startoftext|>', '').replace('\\n', '') # remove extraneous stuff\n",
    "t = t[:t.index('<|Endoftext|>')] # only get one title\n",
    "print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
