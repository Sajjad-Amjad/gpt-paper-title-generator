{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape arxiv and save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxivscraper as ax\n",
    "import numpy as np\n",
    "\n",
    "scraper = ax.Scraper(category='stat', date_from='2017-08-01',\n",
    "                     date_until='2019-07-01', t=10, \n",
    "                     filters={'categories':['stat.ml'],'abstract':['learning']})\n",
    "output = scraper.scrape()\n",
    "\n",
    "\n",
    "# cols = ('id', 'title', 'categories', 'abstract', 'doi', 'created', 'updated', 'authors')\n",
    "titles = [' '.join(o['title'].split()) for o in output]\n",
    "\n",
    "np.savetxt('titles.csv', np.array(titles), fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finetune gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "\n",
    "model_name = \"117M\"\n",
    "gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/117M/\n",
    "\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.finetune(sess,\n",
    "              'titles.csv',\n",
    "              model_name=model_name,\n",
    "              steps=1000,\n",
    "              save_every=200)   # steps is max number of training steps\n",
    "\n",
    "gpt2.generate(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# look at some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "\n",
      "a machine learning framework for computer vision\n",
      "a semi-supervised csi-on-icb test for differential privacy\n",
      "neural control variates for efficient inference and meaningful decision making\n",
      "unlearn what you have learned: recurrent neural networks for pre-processing speech using sparsified emotional speech\n",
      "learning representations for long short-term memory linked index circuits\n",
      "stochastic gradient mcmc for compressing neural network training for training fine-grained relationships\n",
      "bayesian inference and wavelet decomposition of iterative random code\n",
      "robust and parallel k-svm through sparsely correlated matrix decomposition\n",
      "towards a practical $k$-dimensional deep neural network model parameterization theory emphasizing sequence interaction\n",
      "deep signal recovery with dual momentum: a complexity and stability analysis based analysis\n",
      "fault diagnosis using deep signal recovery\n",
      "sparse least squares regression: robust regularization and recommendations for improved statistical validation\n",
      "stacking with a neural network for sequential intent classification\n",
      "deep residual auto-encoders for multi-label image classification\n",
      "sparse least squares regression for robust and adaptive classification\n",
      "anatomical coronary stearage reconstruction using deep 3d convolutional neural networks\n",
      "deep neural processes\n",
      "gated-cgroup communications solution for ciliary dysfunction detection using hierarchical directed acyclic graph convolutional network\n",
      "understanding batch normalization\n",
      "improving gan performance with stochastic gradients and more via optimized alternative estimation\n",
      "graphoisheter encephalization encephalization\n",
      "improving gans using covariate shift and multivariate spatial embeddings\n",
      "dynamic event graph convolutional networks for adverse event forecasting\n",
      "fast asynchronous parallel training of deep networks\n",
      "on the non-parametric power of logistic regression for smooth events\n",
      "unifying pac and learning mdps using influence functions\n",
      "machine learning to plan and downlink using intrinsic motivation\n",
      "classifier readiness testing for imbalanced data\n",
      "fast and scalable bayesian deep learning with limited observations\n",
      "a comparison of deep neural networks and adaptive graph neural networks for anomaly detection\n",
      "distributed deep learning with gossip networks using bidirectional lstm sensors\n",
      "revisiting reuse of super categories\n",
      "anatomical visual exploration\n",
      "multimodal social learning with active interest discovery\n",
      "stochastic variance-reduced cubic regularization for approximate inference\n",
      "predicting county level corn yields based on time series data\n",
      "a deep residual network approach for predicting county level eegs using sparse and incomplete data\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_file = 'samples/samples-901'\n",
    "t = open(sample_file, 'r').read()\n",
    "\n",
    "for s in ['endoftext', 'startoftext', '<|', '|>']:\n",
    "    t = t.replace(s, '')\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Source Separation Via Non-Negative Eigenvector Field Variate Operator\n"
     ]
    }
   ],
   "source": [
    "prefix = 'neural' # None is default\n",
    "text = gpt2.generate(sess,\n",
    "              length=40,\n",
    "              temperature=0.7,\n",
    "              prefix=neural,\n",
    "              nsamples=1,\n",
    "              batch_size=1,\n",
    "              return_as_list=True\n",
    "             )\n",
    "\n",
    "\n",
    "t = text[0].title()\n",
    "t = t.replace('<|Startoftext|>', '').replace('\\n', '') # remove extraneous stuff\n",
    "t = t[:t.index('<|Endoftext|>')] # only get one title\n",
    "print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
